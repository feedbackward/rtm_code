
# mlR_example.R
# Author: Matthew J. Holland
# Updated: 2016/10/12
# Description:
# Simple illustrative examples, executing the proposed method and all benchmark routines.


#### Example run-through. ####

load(file = "./mlR_all.RData") # NOTE: may need to adjust working directory
load(file = "./mlR_data.RData")

## Required packages
require(mnormt)
require(pracma)
require(hypergeo)
require(MASS)
require(quantreg)
require(Rcpp)
require(RcppArmadillo)
require(robustbase)
require(stats)

## Compile C++ source and load R wrappers for main routine.
sourceCpp(file = "./mlC_loc.cpp") # will likely need to update.
sourceCpp(file = "./mlC_scale.cpp") # will likely need to update.


## A simple example, using noise generated by the routine provided.

n_val <- 15
d_val <- 5
level <- 10
X_train <- c()
for (i in 1:d_val){
  X_train <- cbind(X_train, rnorm(n = n_val, mean = 0, sd = 1))
}
w0 <- seq(1, d_val, 1) * rep(c(1,-1), ceiling(d_val/2))[1:d_val] # true weights.
noise_list <- data_noise(n = n_val, num_trials = 1, dist = "laplace", seed = 12345)
noise_nice <- noise_list[[1]][[12]][, 1] # level 12; higher variance.
noise_bad <- noise_list[[1]][[2]][, 1] # level 2; small variance.
y_bad <- drop(X_train %*% w0) + noise_bad
y_nice <- drop(X_train %*% w0) + noise_nice

# Output of our routine.

main_routine(y = y_bad, X = X_train)
main_routine(y = y_nice, X = X_train)

# Output of the competitive benchmarks.

linreg_ols(y = y_bad, X = X_train)
linreg_ols(y = y_nice, X = X_train)

linreg_lad(y = y_bad, X = X_train)
linreg_lad(y = y_nice, X = X_train)

linreg_geomed(y = y_bad, X = X_train)
linreg_geomed(y = y_nice, X = X_train)

linreg_hs(y = y_bad, X = X_train)
linreg_hs(y = y_nice, X = X_train)

linreg_bjl(y = y_bad, X = X_train)
linreg_bjl(y = y_nice, X = X_train)



#### 1-dimensional regression demonstration (Fig. 1 in paper) ####

n_val <- 100
para <- 2.1
fn_true <- function(x){sin(x = x*{2*pi})}
to_centre <- ml_mfrec(shift = 0, scale = 1, shape = para)
x_train <- c(runif(n = n_val/2, min = 0, max = 0.5), runif(n = n_val/2, min = 0.5, max = 1))
noise_train <- c(rnorm(n = n_val/2, mean = 0, sd = 0.5),
                 {ml_rfrec(n = n_val/2, shift = 0, scale = 1, shape = para)-to_centre})
y_train <- fn_true(x = x_train) + noise_train

para_lwd <- 2
para_lty <- 2

#load("./20161018_sample_1dim_data.RData") # nice data!
test_x <- seq(0, 1, 0.01)
plot(x = x_train, y = y_train, type = "p", xlim = c(0,1), xlab = "", ylab = "", pch = 20)
lines(x = test_x, y = fn_true(test_x), col = "gray59")
abline(v = 0.5, col = "gray59")

test_x_1 <- seq(0, 0.5, 0.01)
test_x_2 <- seq(0.5, 1, 0.01)

# The true conditional means.
cond_means_1 <- 0 + fn_true(x = test_x_1)
cond_means_2 <- ml_mfrec(shift = 0, scale = 1, shape = para) + fn_true(x = test_x_2) - to_centre
lines(x = test_x_1, y = cond_means_1, col = "red", lwd = para_lwd)
lines(x = test_x_2, y = cond_means_2, col = "red", lwd = para_lwd)

# True conditional median.
cond_med_1 <- 0 + fn_true(x = test_x_1)
cond_med_2 <- ml_qfrec(p = 0.5, shift = 0, scale = 1, shape = para) + fn_true(x = test_x_2) - to_centre
lines(x = test_x_1, y = cond_med_1, col = "chartreuse4", lwd = para_lwd)
lines(x = test_x_2, y = cond_med_2, col = "chartreuse4", lwd = para_lwd)

# The robust parameter, first the "true scale" assuming we use robust MADmeans, population scale of raw noise.
true_scale_1 <- 0.3371298 # median(abs({rnorm(n = 1e7, mean = 0, sd = 0.5)-0}))
true_scale_2 <- 0.7121748 # median(abs({ml_rfrec(n = 1e7, shift = 0, scale = 1, shape = para)-to_centre-0}))
to_int_1 <- function(x, u, s = true_scale_1){
  dnorm(x = x, mean = 0, sd = 0.5)*ml_psi.gud(x = {x-u}/s)
  # finding the root of this fn in u will result in theta* for the noise *prior to* centering.
  # since theta* is shift equivariant, just shift the estimate by to_centre to get estimate for centered noise.
}
to_int_2 <- function(x, u, s = true_scale_2){
  ml_dfrec(x = x, shift = 0, scale = 1, shape = para)*ml_psi.gud(x = {x-u}/s)
  # finding the root of this fn in u will result in theta* for the noise *prior to* centering.
  # since theta* is shift equivariant, just shift the estimate by to_centre to get estimate for centered noise.
}
to_root_1 <- function(u){
  integrate(f = to_int_1, lower = -Inf, upper = Inf, u = u)$value # domain of integration for Normal noise.
}
to_root_2 <- function(u){
  integrate(f = to_int_2, lower = 0, upper = Inf, u = u)$value # domain of integration for Frechet noise.
}
theta_star_noise_1 <- uniroot(f = to_root_1, interval = c(-100, 100))$root - 0 # note the shift.
theta_star_noise_2 <- uniroot(f = to_root_2, interval = c(0, 100))$root - to_centre # note the shift.
true_gud_1 <- theta_star_noise_1 + fn_true(x = test_x_1)
true_gud_2 <- theta_star_noise_2 + fn_true(x = test_x_2)
lines(x = test_x_1, y = true_gud_1, col = "dodgerblue1", lwd = para_lwd)
lines(x = test_x_2, y = true_gud_2, col = "dodgerblue1", lwd = para_lwd)


# Let's carry out some estimates.

# First specify our model. let's use some polynomials. Assumes x is a (n x d) matrix.
f1 <- function(x, k = 1){
  x[, k]
}
f2_1 <- function(x, k = 1){
  x[, k]*x[, k]
}
f3_1 <- function(x, k = 1){
  x[, k]*x[, k]*x[, k]
}
f4_1 <- function(x, k = 1){
  x[, k]*x[, k]*x[, k]*x[, k]
}
f5_1 <- function(x, k = 1){
  x[, k]*x[, k]*x[, k]*x[, k]*x[, k]
}

features_1 <- function(x){
  cbind(f1(x = x), f2_1(x = x), f3_1(x = x), f4_1(x = x), f5_1(x = x))
}

idx_1 <- 1:n_val/2
idx_2 <- {n_val/2+1}:n_val

# First, OLS. Seeks an estimate of conditional mean, i.e. mean of p(y;x).
w_ols_1 <- linreg_ols(y = y_train[idx_1], X = features_1(x = t(t(x_train[idx_1]))))
w_ols_2 <- linreg_ols(y = y_train[idx_2], X = features_1(x = t(t(x_train[idx_2]))))

cond_mean_est_1 <- drop(features_1(x = t(t(test_x_1))) %*% w_ols_1)
cond_mean_est_2 <- drop(features_1(x = t(t(test_x_2))) %*% w_ols_2)

lines(x = test_x_1, y = cond_mean_est_1, col = "firebrick", lwd = para_lwd, lty = para_lty)
lines(x = test_x_2, y = cond_mean_est_2, col = "firebrick", lwd = para_lwd, lty = para_lty)

# Next, LAD. Seeks an estimate of  conditional median, i.e. median of p(y;x)
w_lad_1 <- linreg_lad(y = y_train[idx_1], X = features_1(x = t(t(x_train[idx_1]))))
w_lad_2 <- linreg_lad(y = y_train[idx_2], X = features_1(x = t(t(x_train[idx_2]))))

cond_med_est_1 <- drop(features_1(x = t(t(test_x_1))) %*% w_lad_1)
cond_med_est_2 <- drop(features_1(x = t(t(test_x_2))) %*% w_lad_2)

lines(x = test_x_1, y = cond_med_est_1, col = "olivedrab4", lwd = para_lwd, lty = para_lty)
lines(x = test_x_2, y = cond_med_est_2, col = "olivedrab4", lwd = para_lwd, lty = para_lty)

# Finally, a robust target minimizer.
w_gud_est_1 <- main_routine(y = y_train[idx_1], X = features_1(x = t(t(x_train[idx_1]))),
                               alt.psiwls = ml_wt.gud, altC.est = est_gud, altC.scale = scale_madmean)
w_gud_est_2 <- main_routine(y = y_train[idx_2], X = features_1(x = t(t(x_train[idx_2]))),
                               alt.psiwls = ml_wt.gud, altC.est = est_gud, altC.scale = scale_madmean)

cond_gud_est_1 <- drop(features_1(x = t(t(test_x_1))) %*% w_gud_est_1)
cond_gud_est_2 <- drop(features_1(x = t(t(test_x_2))) %*% w_gud_est_2)

lines(x = test_x_1, y = cond_gud_est_1, col = "dodgerblue3", lwd = para_lwd, lty = para_lty)
lines(x = test_x_2, y = cond_gud_est_2, col = "dodgerblue3", lwd = para_lwd, lty = para_lty)
legend(x = "topleft", legend = c("mean", "med", "new", "mean_est", "med_est", "new_est"),
       lty = c(1, 1, 1, para_lty, para_lty, para_lty), lwd = para_lwd, cex = 0.75, ncol = 2,
       col = c("red", "chartreuse4", "dodgerblue1", "firebrick", "olivedrab4", "dodgerblue3"))







